---
title: "6월 28일 과제" 
author: "황 성 윤"
date: '2019 6 28 '
output: html_document
---


## 1. 부모의 키가 클수록 자식의 키도 상대적으로 크다고 하는데, 아버지의 키와 아들의 키를 조사하였더니 아래와 같이 나왔다고 한다. 이 자료를 바탕으로 해서 회귀식을 구하고 아버지의 키가 165cm일 때 아들의 키는 얼마인지 예측하시오.
```{r}
x <- c(150,160,170,180,190)
y <- c(176,179,182,181,185)
(parents <- data.frame(x=x,y=y))
(fit <- lm(y~x,data=parents))
summary(fit)
146.6+165*0.2
```
회귀식 : y.hat=146.6+0.2*x <br>
정답 : 약 179.6cm


## 2. 소득이 높을수록 신용카드 사용량이 많아진다고 하는데, 월 소득 대비 신용카드 사용량을 조사하였더니 아래와 같이 나왔다고 한다. 이 자료를 바탕으로 해서 회귀식을 구하고, 월 소득이 250만원일 때 신용카드 사용량을 예측하시오. (단위: 만원)
x : 월 소득, y : 신용카드 사용량
```{r}
x <- c(100,200,300,400,500)
y <- c(30,70,85,140,197)
(credit <- data.frame(x=x,y=y))
(fit <- lm(y~x,data=credit))
summary(fit)
-16.8+0.404*250
```
회귀식 : y.hat=-16.8+0.404*x <br>
정답 : 약 84만2천원


## 3. mtcars 데이터셋에서 배기량(disp)에 따른 마력(hp)의 회귀식을 구하시오.
```{r}
mtcars <- mtcars
attach(mtcars)
(fit <- lm(hp~disp,data=mtcars))
summary(fit)
```
회귀식 : hp.hat=45.7345+0.4375*disp

## 4. MASS 패키지를 설치하고, 이 패키지 안에 있는 Boston 데이터셋을 이용하여 Boston 인근의 집값을 결정하는 다중회귀 모델을 만드시오.

### 모형식 : yi=b0+b1*x1i+b2*x2i+...+bn*xni+ei, (i=1,2,...,p, ei ~ i.i.d N(0,sigma^2))
### response variable : medv

#### CRIM: 자치시(town) 별 1인당 범죄율
#### ZN: 25,000 평방피트를 초과하는 거주지역의 비율
#### INDUS: 비소매상업지역이 점유하고 있는 토지의 비율
#### CHAS: 찰스강에 대한 더미변수(강의 경계에 위치한 경우는 1, 아니면 0)
#### NOX: 10ppm 당 농축 일산화질소
#### RM: 주택 1가구당 평균 방의 개수
#### AGE: 1940년 이전에 건축된 소유주택의 비율
#### DIS: 5개의 보스턴 직업센터까지의 접근성 지수
#### RAD: 방사형 도로까지의 접근성 지수
#### TAX: 10,000 달러 당 재산세율
#### PTRATIO: 자치시(town)별 학생/교사 비율
#### B: 1000(Bk-0.63)^2, 여기서 Bk는 자치시별 흑인의 비율을 말함.
#### LSTAT: 모집단의 하위계층의 비율(%)
#### MEDV: 본인 소유의 주택가격(중앙값) (단위: $1,000)

#### 출처 : https://statkclee.github.io/deep-learning/r-keras-boston-housing.html 

분석을 위해 더미변수인 chas는 factor로 처리한다.
```{r}
library(MASS)
Boston <- Boston
head(Boston)
Boston$chas <- as.factor(Boston$chas)
str(Boston)
```
먼저 산점도 행렬을 그려보면 다음과 같다.
```{r}
pairs(Boston)
```
산점도를 통해서 봤을 때, 반응변수인 medv와 설명변수의 후보가 될 나머지 변수들 사이의 관계가 어느 정도 존재하는 것으로 보여진다. 먼저 모든 후보 변수들을 사용하여 선형회귀모형 fit1을 구축한다.

### 1. Linear Regression Model
```{r}
(fit1 <- lm(medv~.,data=Boston))
summary(fit1)
anova(fit1)
```
위의 분산분석표를 통해서 봤을 때, 변수 nox와 rad는 유의미한 영향력이 없는 것으로 보여진다. 이제 stepwise regression 방법과 all possible regression 방법을 이용해 적절한 변수를 찾아보도록 하겠다. 

#### Stepwise regression

##### 1. backward elimination
이 방법은 모든 설명변수를 포함한 모형에서 시작하여 타당성이 부족한 모형을 하나씩 제거해 나가는 방법을 이용한다. 이에 대한 기준이 되는 통계량은 AIC(Akaike's An Information Criterion)이다. 즉, AIC의 값이 작을수록 더 바람직한 모형임을 암시한다.
```{r}
step(fit1,direction="backward")
```
분석결과, 변수 age와 indus가 영향력이 없는 변수로 판단되어 제거되었다.

##### 2. forward selection
이 방법은 모든 설명변수를 포함하지 않은 모형부터 시작해서 설명변수를 하나씩 추가하면서 모형의 타당성을 판단한다. 역시 이에 대한 기준은 AIC이다.
```{r}
fit2 <- lm(medv~1,data=Boston)
step(fit2,direction="forward",scope=~crim+zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat)
```
분석결과, 변수 indus와 age를 제외한 모든 변수가 선택되었다. 결과적으로 backward elimination과 동일한 결론을 주고 있다.

##### 3. stepwise regression
이 방법은 backward elimination과 forward selection을 결합한 방법으로 변수를 제외하거나 추가시키는 과정을 동시에 하면서 모형의 타당성을 판단한다. 역시 이에 대한 기준은 AIC이다. 
```{r}
step(fit1,direction="both")
```
분석결과, 변수 indus와 age를 제외한 모든 변수가 선택되었다. 결과적으로 backward elimination, forward selection, 그리고 stepwise regression 모두 동일한 결론을 주고 있다. 이 모형을 fit2라 하자.
```{r}
(fit2 <- lm(medv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+black+lstat,data=Boston))
summary(fit2)
```
해당 anova table을 보면 설명변수 모두가 유의미한 변수라는 결과를 주고 있다.

#### 4. All possible regression
이 방법은 모든 가능한 회귀모형을 구축하고 이 중 가장 적절한 모형을 선택한다. 선택 기준은 BIC(Bayesian Information Criterion)이고, AIC와 마찬가지로 BIC의 값이 가장 작은 모형을 최적의 결과로 선택한다.
```{r}
library(leaps)
subsets1 <- regsubsets(medv~.,data=Boston,
                      method='seqrep',nbest=13) 
plot(subsets1)
subsets2 <- regsubsets(medv~.,data=Boston,
                      method='exhaustive',nbest=13)
plot(subsets2)
```
분석결과, 두가지 방법 모두 변수 crim, indus, age, rad, tax를 제외한 모형을 추천하고 있다. 이 모형을 fit3이라 하자.
```{r}
(fit3 <- lm(medv~zn+chas+nox+rm+dis+ptratio+black+lstat,data=Boston))
summary(fit3)
```
해당 anova table을 보면 설명변수 모두가 유의미한 변수라는 결과를 주고 있다.

### 2. Checking Assumptions
이제 모형 fit2와 fit3을 구축하는 것이 타당한지 판단해본다. 모형식에서도 알 수 있듯이 다중선형회귀모형을 구축시에는 잔차에 대한 다음과 같은 가정을 하게 된다. <br>
1. 정규성 (normality) <br>
2. 등분산성 (homoscedasticity) <br>
3. 선형성 (linearity) <br>
4. 독립성 (indepandence) <br>

#### fit2 (not include indus, age)
먼저 잔차(residuals)와 관련한 4가지의 그림을 통해 전체적으로 기본가정 충족여부를 파악해본다.
```{r}
par(mfrow=c(2,2))
plot(fit2)
par(mfrow=c(1,1))
```
위의 그래프를 통해 파악해 본 결과, 정규성과 선형성에 대한 의심이 든다. 그리고 369, 372, 373번째의 관측치가 이상치(outlier)라고 판단된다. 

#### 1) 정규성 검정 (Shapiro-Wilk Normality Test)
먼저 잔차에 대한 정규성을 Shapiro-Wilk 검정법을 통해 실시해본다. 
```{r}
qqnorm(fit2$residuals) ; qqline(fit2$residuals)
shapiro.test(fit2$residuals)
```
분석결과, p-value의 값이 유의수준 0.05보다 매우 작은 값이다. 그러므로 정규성에 의심이 가는 상황이다. 

#### 2) 선형성, 등분산성 검정 (package gvlma)
여기에서는 패키지 gvlma에 있는 함수 gvlma를 이용해 검정해보도록 하겠다. 실행결과에서 Global test는 선형성에 대한 검정이고 Heteroscedasticity는 등분산성에 대한 검정이다.
```{r}
library(gvlma)
gvmodel2 <- gvlma(fit2)
summary(gvmodel2)
```
검정결과, 두가지 검정 모두 p-value값이 유의수준 0.05보다 매우 작다. 따라서 선형성, 등분산성 모두 의심이 가는 상황이다. 

#### 3) 독립성 검정 (Durbin-Watson Test)
독립성 검정에는 보통 Durbin-Watson 검정법을 사용하게 되며, 다음과 같은 기준으로 독립성을 판단한다. <br>
DW statistic == 2 --> 독립 (row==0)
DW statistic == 0 --> 양의 자기상관 (row==1) 
DW statistic == 4 --> 음의 자기상관 (row==-1)
```{r}
library(car)
durbinWatsonTest(fit2)
```
검정결과, p-value의 값이 매우 작게 나왔다. 그러므로 독립성 가정에도 의심이 가게 된다. 따라서 모형 fit2는 기본가정을 만족하지 못하는 것으로 결론내린다. <br>

#### fit3 (not include crim, indus, age, rad, tax)
방법은 fit2의 경우와 동일하다.
```{r}
par(mfrow=c(2,2))
plot(fit3)
par(mfrow=c(1,1))
```
위의 그래프를 통해 파악해 본 결과, 정규성과 선형성에 대한 의심이 든다. 그리고 369, 372, 373번째의 관측치가 이상치(outlier)라고 판단된다. 

#### 1) 정규성 검정 (Shapiro-Wilk Normality Test)
```{r}
qqnorm(fit3$residuals) ; qqline(fit3$residuals)
shapiro.test(fit3$residuals)
```
분석결과, p-value의 값이 유의수준 0.05보다 매우 작은 값이다. 그러므로 정규성에 의심이 가는 상황이다. 

#### 2) 선형성, 등분산성 검정 (package gvlma)
```{r}
library(gvlma)
gvmodel3 <- gvlma(fit3)
summary(gvmodel3)
```
검정결과, 두가지 검정 모두 p-value값이 유의수준 0.05보다 매우 작다. 따라서 선형성, 등분산성 모두 의심이 가는 상황이다. 

#### 3) 독립성 검정 (Durbin-Watson Test)
```{r}
library(car)
durbinWatsonTest(fit3)
```
검정결과, p-value의 값이 매우 작게 나왔다. 그러므로 독립성 가정에도 의심이 가게 된다. 따라서 모형 fit3는 기본가정을 만족하지 못하는 것으로 결론내린다.

#### 1차 결론
결과적으로 모형 fit2와 fit3 모두 다중선형회귀분석에 대한 기본가정을 만족하지 못하는 것으로 풀이된다. 다만 AIC를 계산해보면 다음과 같다.
```{r}
AIC(fit2,fit3)
```
AIC를 계산해본 결과 fit3보다는 fit2가 더 바람직한 모형으로 여겨진다. 다만, 데이터 Boston에서 집값을 결정하는 모형들 중 굳이 다중선형회귀모형을 선택하라고 한다면 indus와 age를 제외한 모든 변수를 사용하는 모형을 선택할 수 있겠다. 하지만, 더 타당성 있는 모형을 찾고자 한다면 비선형모형이나, polynomial regression, 변수변환 등의 방법을 생각해볼 수 있겠다. 이전에 잔차도를 통해서도 알 수 있지만 모형을 구축 시 선형성이 의심이 간 상황이므로 의심가는 변수에 대해 2차항을 추가로 넣어볼 수 있겠다. 의심가는 변수를 확인하기 위해 Component+Residual (Partial Residual) Plots 을 그려보도록 하겠다.
```{r}
library(car)
crPlots(fit2)
```
검토결과, 변수 crim, rm 그리고 lstat에 대해 선형성이 의심이 가므로 이 네가지 변수에 대한 2차항을 추가로 넣어서 모형 fit4를 구축해보도록 하겠다.

### 3. Polynomial Regression
```{r}
(fit4 <- lm(medv~crim+I(crim^2)+zn+chas+nox+rm+I(rm^2)+dis+rad+tax+ptratio+black+lstat+I(lstat^2),data=Boston))
summary(fit4)
```
분석결과, 변수 zn이 유의미하지 않게 나오므로 이 변수를 제거한 모형을 구축해본다.
```{r}
(fit4 <- lm(medv~crim+I(crim^2)+chas+nox+rm+I(rm^2)+dis+rad+tax+ptratio+black+lstat+I(lstat^2),data=Boston))
summary(fit4)
```
분석결과, 모든 설명변수가 유의미하다고 보여진다. 이제 모형 fit5가 기본가정을 만족하는 지 그래프를 통해 전체적으로 살펴보도록 한다.
```{r}
par(mfrow=c(2,2))
plot(fit4)
par(mfrow=c(1,1))
```
분석결과, 표준화잔차(standardized residuals)나 정규성 가정여부, 그리고 leverage의 관점으로 살펴봤을 때 366, 369, 372, 373 번째에 해당하는 관측치가 이상치(outlier)라고 판단되므로 이 네가지의 관측치를 제거한 다음 모형 fit5를 구축해서 살펴보도록 하겠다.
```{r}
Boston1 <- Boston[-c(366,369,372,373)] 
(fit5 <- lm(medv~crim+I(crim^2)+chas+nox+rm+I(rm^2)+dis+rad+tax+ptratio+black+lstat+I(lstat^2),data=Boston1))
summary(fit5)
par(mfrow=c(2,2))
plot(fit5)
par(mfrow=c(1,1))
```
다중회귀의 기본가정 성립을 방해하는 이상치를 제거해도 또다른 이상치가 생기는 것으로 보인다. 그렇기에 이 정도를 끝으로 모형구축을 종료하도록 한다. <br>
하지만, 여기에서 간과하지 말아야할 게 있다. 바로 다중공선성의 문제(multicollinearity problem)이다. 즉, 사용하는 설명변수들끼리의 연관성이 존재한다면 분석 시 문제가 될 수 있기 때문에 분산팽창계수(Variance Inflation Factor, VIF)를 통해 이를 점검해보도록 하겠다. 보통 VIF의 값이 10을 넘는다면 다중공선성의 문제가 있다고 판단하게 된다.

#### Multicollinearity Problem
```{r}
require(car)
vif(fit5)
vif(fit5)>10
```
분석결과, 변수 crim과 2차항이 포함된 변수들 모두가 다중공선성의 문제가 있다고 나온다. 이는 당연한 결과이다. 이를 보완하기 위해 표준화(standardized) 작업을 실시하면 어느정도 문제점을 보완할 수 있을 것으로 여겨진다. 다만 여기에서 명목변수인 chas의 경우에는 표준화 작업이 불가능하므로 이 변수는 제외하고 실시하겠다. 

#### standardized
```{r}
Boston2 <- Boston1[,-4]
Boston2 <- as.data.frame(scale(Boston2))
(fit6 <- lm(medv~crim+I(crim^2)+nox+rm+I(rm^2)+dis+rad+tax+ptratio+black+lstat+I(lstat^2),data=Boston2))
summary(fit6)
require(car)
vif(fit6)
vif(fit6)>10
```
평균중심화 작업 결과, 변수 crim만 VIF의 값이 10보다 크고 나머지 변수는 기준을 만족하고 있다. 따라서 변수 crim만 제거하고 다시 모형을 구축해서 살펴보도록 하겠다.
```{r}
(fit6 <- lm(medv~nox+rm+I(rm^2)+dis+rad+tax+ptratio+black+lstat+I(lstat^2),data=Boston2))
summary(fit6)
require(car)
vif(fit6)
vif(fit6)>10
```
분석결과, 포함된 모든 변수의 VIF값이 10보다 작으므로 다중공선성의 문제에서 어느정도 벗어난 것으로 판단된다. 이에 따라 모형 fit6을 최선의 결과로 판단하고자 한다. 만약 polynomial regression보다 더 좋은 결과를 얻고자 한다면 반응변수를 적절한 기준에 의해 명목변수로 바꾼 뒤 Logistic Regression을 적용해보거나, 각 설명변수에 맞는 변환함수(transformation function)을 설정하여 모형을 구축해야 하는데, 이는 상당히 비효율적일 뿐만 아니라 최적의 변환함수라는 것이 연구자마다 다르게 나올 수 있기 때문에 비추천하고 싶다. 대신 데이터의 특성에 상관없이 해당 데이터의 특성에 맞게 데이터를 적절한 형태로 변환해주는 Kernel-trick이라는 방법이 있음을 명시하고 싶다. 이에 대해서는 아래의 논문을 참고하기 바란다.

link1 : http://www.csam.or.kr/journal/view.html?doi=10.5351/CSAM.2015.22.2.201 <br>
link2 : http://www.csam.or.kr/journal/view.html?doi=10.5351/CSAM.2016.23.4.355


